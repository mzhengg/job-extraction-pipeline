# Skills Gap Dashboard

This is an end-to-end project showcasing my skills as an aspiring Software Engineer (Data Engineer).  

Software engineering is one of the most in-demand, highest paying jobs currently. There's lots of room for growth and the industry is expected to grow by 25% through the next decade according to the Bureau of Labor Statistics (https://www.bls.gov/ooh/computer-and-information-technology/software-developers.htm). But this industry is constantly changing and new tools are being developed all the time. This project aims to track the most sought after technologies by the industry so that aspiring software engineers can keep up to date with them.  

## Technologies:

* Python (Selenium, Boto3, PySpark)
* AWS (S3, Lambda, EC2, Redshift, Elastic Beanstalk)
* Airflow
* Docker
* Terraform

## Overview of Terraform

An infrastructure-as-code (the process of managing and provisioning computer data centers through code instead of manual processes) tool that lets you define both cloud and on-premise resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle.

## Overview of Docker

Docker is a platform that enables developers to build, deploy, run, update, and manage containers.  

Containers: package of software that includes everything needed to run an application: code, dependencies, etc.  

Containers are isolated from each other and can be ran in different environments (Windows, macOS, GCP, etc.)  

Allows:
* Reproducibility 
* Local experiments 
* Integration testing 
* Running pipelines on the cloud 
* The use of Spark 
* Serverless computing 

Dockerfile: a text file you create that builds a Docker image  

Docker image: a file containing instructions to build a Docker container  

Docker container: a running instance of a Docker image
* Since a Docker image is just a snapshot, any modifications performed in the container will be lost upon restarting the container  

Dockerfile -> (Build) -> Docker image -> (Run) -> Docker container  

Docker compose (docker-compose.yml): a file to deploy, combine, and configure many Docker images at once  

Docker volume: file system mounted on Docker container to preserve data generated by the running container (stored on the host, independent of the container life cycle  

“Docker rule” is to outsource every process to its own container  

## Overview of Airflow

Apache Airflow is the most popular data workflow orchestration tool.   

# 1) Setup Terraform
- Open terraform folder and follow the instructions in README.md

# 2) Setup for AWS S3 storage (data lake)
- Open the Makefile, enter your AWS details on lines 11-13

AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
AWS_DEFAULT_REGION=us-west-2

# 3) Test scraper in local Lambda environment
(where Makefile and Dockerfile are located)

1. make docker-build

2. make docker-run

3. make docker-test

4. If previous step successful: make build-lambda-package

# 4) Deploy code to AWS Lambda (web scraping)
# ref: https://docs.aws.amazon.com/lambda/latest/dg/images-create.html

- Go to AWS Lambda page > Create Function > Author from scratch

- Function-name: indeed-web-scraper, Runtime: Python, Architecture: x86_64, Execution Role: Create a new existing role from AWS policy templates, Role name: scraper, Policy templates: Basic Lambda@Edge permissions

- Add trigger > CloudWatch Events > Create a new rule

- Rule name: dailyscraper, Rule description: Run at 12 p.m. EST every day, Rule type: Schedule Expression, Schedule Expression: cron(0 18 ? * MON-FRI *)

- Go to 'Configuration' tab > Edit > Add Environmental Variable x2

- PATH = /var/task/bin, PYTHONPATH = /var/task/src:/var/task/lib

- Go to 'General configuration' > Edit > Memory: 1200MB

- Go to IAM > Roles > 'scraper' > Add permissions > Attach policies > 'AmazonS3FullAccess'

- Go to Upload from > .zip file > upload 'build.zip' and save

- Locate Runtime settings > Edit > Handler: etl.indeed_scraper

- How do we run it??? Under 'Test'?