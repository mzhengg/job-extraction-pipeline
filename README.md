# Job Extraction Pipeline

Software engineering is one of the most in-demand, highest paying jobs currently. There's lots of room for growth and the industry is expected to grow by 25% through the next decade according to the Bureau of Labor Statistics (https://www.bls.gov/ooh/computer-and-information-technology/software-developers.htm). But this industry is constantly changing and new positions are opening up all the time. This project aims to track the number of positions open for the various specialities within software engineering so that aspiring software engineers can pivot into growing fields, learn the necessary skills, and get a job with a good growth outlook.

## Technologies:

* Python (Selenium, Boto3)
* AWS (S3, Fargate, MWAA, Redshift)
* Airflow
* Docker
* Terraform

![My_Image](architecture-diagram.jpg)

## Overview of Terraform

An infrastructure-as-code (the process of managing and provisioning computer data centers through code instead of manual processes) tool that lets you define both cloud and on-premise resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle.

## Overview of Docker

Docker is a platform that enables developers to build, deploy, run, update, and manage containers.  

Containers: package of software that includes everything needed to run an application: code, dependencies, etc.  

Containers are isolated from each other and can be ran in different environments (Windows, macOS, GCP, etc.)  

Allows:
* Reproducibility 
* Local experiments 
* Integration testing 
* Running pipelines on the cloud 
* The use of Spark 
* Serverless computing 

Dockerfile: a text file you create that builds a Docker image  

Docker image: a file containing instructions to build a Docker container  

Docker container: a running instance of a Docker image
* Since a Docker image is just a snapshot, any modifications performed in the container will be lost upon restarting the container  

Dockerfile -> (Build) -> Docker image -> (Run) -> Docker container  

Docker compose (docker-compose.yml): a file to deploy, combine, and configure many Docker images at once  

Docker volume: file system mounted on Docker container to preserve data generated by the running container (stored on the host, independent of the container life cycle  

“Docker rule” is to outsource every process to its own container  

## Overview of Airflow

Apache Airflow is the most popular data workflow orchestration tool.   

## How to Setup and Deploy Dashboard

### 1) Setup Infrastucture using Terraform

Terraform is used to set up the AWS infrastructure (S3, Fargate, MWAA, Redshift).  

#### Steps

1. Get AWS keys by following these steps:
    - sign into AWS console
    - click on username
    - click on security credentials
    - create access key

2. Set these environment variables (these are dummy keys, replace them with your own before executing):

```bash
export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
export AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY  
```

3. Build infrastructure by executing these commands in order:
```bash
terraform init
```
```bash
terraform plan
```
*When you build a redshift cluster, you will be prompted to make a master username and password for accessing the cluster. Store this information in a secure place as you will need to access this again later. I've chosen to make a .env file in the working directory, that I do not upload to the repo for security reasons, to store information about my AWS access keys and Redshift credentials.*
```bash
terraform apply
```

4. Destroy the infrastructure (when they are no longer needed):
```bash
terraform destroy
```

If any issues arise when you try to destroy the infrastructure using the above command, then go into the AWS console and destroy them manually. However, if you do this, then you have to delete all the files in the `terraform` folder except for `main.tf` and `variables.tf` to remove any history of previous infrastructure builds.  

Now, go to AWS Redshift in the console and locate your cluster. Retrieve information about the database name, port, and host name (endpoint URL - remove the database and port from the URL) and add it to the .env because it will be used in the next part  

IMPORTANT: When you create an Amazon Redshift cluster, it is locked down by default so nobody has access to it. To grant other users inbound access to a Redshift cluster, you associate the cluster with a security group. To do this, follow these steps:  

* Go to the AWS Console -> Redshift -> Cluster and click on the VPC security group
* Click on `Inbound rules` and then click `Edit Inbound rules`
* `Add rule` and select the following for the new rules: Type = `All traffic`, Source: `0.0.0.0/0`.  

And that's it, the Redshift cluster should now be accessible via pyscopg2 in the following part.  

Now, you must create an IAM user in order to run queries in a Redshift cluster. To do this, follow these steps:

* Go to the AWS Console -> Redshift -> Clusters -> Associated IAM roles
* Click on `Manage IAM roles`, select `Any S3 bucket`, and select `Create IAM role as default`
* This IAM role will have full command access to Redshift and be able to access any S3 bucket (generally this is not recommended, but for the purposes of this project it is fine)

Redshift is now fully setup!

### 2) Test and Deploy Container to AWS Fargate

AWS Fargate is used to host and execute a container that will:
    - Scrape Indeed.com using Selenium and Python
    - Upload the raw job postings to AWS S3
    - Fix structural errors in the raw data
    - Store the structured data in AWS Redshift  

By default, the script only scrapes one type of job: Software Engineer. In the future, there is room to scrape other jobs as well! But for now, we will scrape 10 pages of that one job every week.  

At first, I intended to use AWS Lambda to trigger the pipeline to run. However, I realized this wasn't necessary and it only added an extra layer of complexity. I encountered a lot of difficulty running headless Chrome in a container. After days of searching, I stumbled across this repo: https://github.com/umihico/docker-selenium-lambda. The repo provides a Dockerfile that builds a container that runs Chrome using the AWS Lambda RIE (Runtime Interface Emulator). Even though I won't be using AWS Lambda, this Dockerfile provides a convenient way to Dockerize my selenium web crawler for deployment to AWS Fargate.

First the `get_page_links` function is executed which takes 3 parameters: job, location, and number of pages. It takes these parameters and generates the Indeed links to each page that is to be scraped.  

Within each page or the links to each job posting, thus the page links retrieved above are then given to the `get_job_links` function to grab each job link.  

The job links are given to `upload_to_s3_and_transform` to do 3 things. Firstly, the function will iterate through each job link and call the `scraper` function to extract all the text from the given job post. It will save the job post as a text file to an s3 bucket under the raw/ folder as these have not been pre-processed. Then, the job title from the job post is extracted and the date that the post is scraped is appended to a dataframe that will be saved as a csv containing all the job posts to an s3 bucket under the processed/ folder. To put an object, the text files, to s3 was straight forward. All that was needed was a put_object function. However, to put a csv file required me to first store the csv (pandas dataframe) in a temporary location using StringIO() and then retrieve that value when calling the put_object function to upload it to s3.

Now that all the data is stored in s3, we call `s3_to_redshift` to transfer only the csv files to redshift to make the data queryable and retrievable by the Metabase dashboard.

AWS Redshift is essentially a PostgreSQL database. Thus, I use psycopg2 to connect to the redshift cluster. In order to do this, we need to give it 5 parameters: database name, port, master username, master password, and the host name. These parameters are located in the .env file that we generated earlier.

The username, password, and AWS access keys cannot be exposed for security reasons. Therefore, I pass them in along with the other information needed to connect to the databse when I run the container.

#### Steps

1. Build the image:
```bash
docker build . \
	--tag scraper:latest
```

2. Run the image (these are dummy keys and credentials, replace them with your own before executing using the information contained in the .env file)
```bash
docker run \
    -p 9000:8080 \
    --name scraper \
    -e AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE \
    -e AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY \
    -e AWS_DEFAULT_REGION=us-west-2 \
    -e AWS_REDSHIFT_MASTER_USERNAME=EXAMPLE_USERNAME \
    -e AWS_REDSHIFT_MASTER_PASSWORD=EXAMPLE_PASSWORD \
    -e AWS_REDSHIFT_DATABASE_NAME=DATABASE \
    -e AWS_REDSHIFT_PORT=PORT \
    -e AWS_REDSHIFT_HOST=HOST \
    -e AWS_S3_BUCKET_NAME=BUCKET_NAME \
    scraper:latest
```

3. Delete the image and container (when they are no longer needed)
```bash
docker kill scraper
docker rm scraper
docker image rm scraper:latest
```

### 3) Test and Deploy Airflow DAG to AWS MWAA

Airflow is used to orchestrate the lambda function. Every week, the DAG will trigger the data pipeline to scrape Indeed for new Software Engineering job postings. Each job posting will be saved as a .txt file in an S3 bucket. The S3 bucket will serve as a data lake. Then the raw data will be transformed using PySpark and uploaded to AWS Redshift. The DAG was developed and tested on a local machine using https://github.com/aws/aws-mwaa-local-runner. Then, the DAG was deployed to AWS MWAA.

#### Steps

1. Build the Docker container image using the following command:
```bash
./mwaa-local-env build-image
```

2. Runs a local Apache Airflow environment that is a close representation of MWAA by configuration.
```bash
./mwaa-local-env start
```

*To stop the local environment, Ctrl+C on the terminal and wait until the local runner and the postgres containers are stopped.*

3. Access the Airflow UI

By default, the `bootstrap.sh` script creates a username and password for your local Airflow environment.

- Username: `admin`
- Password: `test`
- Open the Apache Airlfow UI: <http://localhost:8080/>.

4. Add DAGs and supporting files

- Add DAG code to the `dags/` folder.
- Add Python dependencies to `requirements/requirements.txt`.
    * To test a requirements.txt without running Apache Airflow, use the following script:
    ```bash
    ./mwaa-local-env test-requirements
    ```
- Add custom plugins to the `plugins/` folder.